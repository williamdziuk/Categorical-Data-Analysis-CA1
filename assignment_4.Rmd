---
title: "assignment 4"
author: "Andri Þór Stefánsson & Will Dziuk"
date: "2025-12-11"
output: pdf_document
---

```{r echo=FALSE, message=FALSE}
library(knitr)
library(dplyr)
library(ResourceSelection)
library(pROC)
set.seed(2025)
#setwd("~/projects/uni/categorical_data_analysis/Categorical-Data-Analysis-CA1")
data4 <- read.csv("data_ca4.csv")
```

## 1.

Data was loaded, and group v5 (Ethnicity) was combined, from 3 categories to 2, due to low cell counts.
Group 21 (Consciousness level) was also combined from 3 categories to 2 for the same reason.
```{r}
data4$v5[data4$v5>1] <- 0
data4$v21[data4$v21>1] <- 1
```

We use backwards elimination to select a model, starting with the full model, and removing interaction terms until we find the best model.

Note that we remove v1 (Patient ID) from the model full model.

```{r}
m_full <- glm(v2~.-v1, family=binomial,data=data4)
chosen_model <- step(m_full, direction="backward", trace = FALSE)
```

From the following model, we can calculate the ODDS ratio from parameters by using the formula: $$\theta_i = e^\beta_i$$.
The Confidence intervals can be found using Wald Intervals: 
$$CI = [e^{\beta_i - 1.96\cdot SE_i}, e^{\beta_i + 1.96\cdot SE_i}]$$

The factors that affect the probability are listed in the following table, ordered by significance level. The odds and odds confidence intervals are also listed in the table.
```{r echo=FALSE}
coefs <- summary(chosen_model)$coef
variables  <- c("(Intercept)", "Age", "Cancer", "Blood Pressure", "Type of Admission", "Blood PH", "Blood CO2", "Conciousness level")
estimate <- coefs[,1]
std_error <- coefs[,2]
p_value <- coefs[,4]
OR <- exp(estimate)
OR_lower <- exp(estimate - qnorm(0.975)*std_error)
OR_upper <- exp(estimate + qnorm(0.975)*std_error)
results <- data.frame(estimate, std_error, p_value, OR, OR_lower, OR_upper, row.names=variables)
results <- results %>% arrange(p_value)
kable(results)
```

As an example, this shows us that the odds of dying are $89.6783468$ (CI = [12.2868176, 654.5393706]) higher when you are not awake, compared to when you are awake.

All other categorical factors can be interpreted in the same way, using the odds ratio and confidence intervals.

The age coefficient tells us that the odds of dying are multiplied by $1.0436083$ (CI = [1.0159449,	1.0720251]) for each year increase of the patient's age.

The blood pressure coefficient is the only other continuous variable in the model, and can be interpreted in the same way, with the odds of dying being multiplied by $0.9855667$ (CI=	[0.9719801,	0.9993434]) by each unit increase in blood pressure.


## 2. 
```{r}
model_predictions <- predict(chosen_model, type="response")
hoslem.test(
  data4$v2,
  model_predictions,
  g=10
)
```
In a Hosmer and Lemeshow test we test the null hypothesis that the model fits the data.
From this, we get a p-value of $0.5131$, meaning that we fail to reject the null hypothesis.

## 3. 
```{r}
pred01_full <- ifelse(model_predictions > 0.5, 1, 0)
confmatrix_1 <- table(1-data4$v2,1-pred01_full)
dimnames(confmatrix_1) <- list(Actual = c("Not-survive", "Survive"),
Predicted = c("Not-survive", "Survive"))
confmatrix_1
```


```{r echo=FALSE}
n <- sum(confmatrix_1)
n1_1 <- sum(confmatrix_1[1,])
n2_1 <- sum(confmatrix_1[2,])
accuracy_1 <- (confmatrix_1[1,1]+confmatrix_1[2,2])/n
sensitivity_1 <- confmatrix_1[1,1]/n1_1
specificity_1 <- confmatrix_1[2,2]/n2_1
cat("accuracy: ", accuracy_1)
cat("sensitivity: ", sensitivity_1)
cat("specificity: ", specificity_1)
```
From this, we see that the specificity is high, but the sensitivity is low.
From this, we can try lowering the threshold, which should result in more predictions of not surviving, heightening the sensitivity.
```{r}
pred02 <- ifelse(model_predictions > 0.25, 1, 0) # Binary classification, cut-off = 0.5
confmatrix_2 <- table(1-data4$v2,1-pred02)
dimnames(confmatrix_2) <- list(Actual = c("Not-survive", "Survive"),
Predicted = c("Not-survive", "Survive"))
confmatrix_2
```


```{r echo=FALSE}
n <- sum(confmatrix_2)
n1_2 <- sum(confmatrix_2[1,])
n2_2 <- sum(confmatrix_2[2,])
accuracy_2 <- (confmatrix_2[1,1]+confmatrix_2[2,2])/n
sensitivity_2 <- confmatrix_2[1,1]/n1_2
specificity_2 <- confmatrix_2[2,2]/n2_2
cat("accuracy: ", accuracy_2)
cat("sensitivity: ", sensitivity_2)
cat("specificity: ", specificity_2)
```

```{r}
pred03 <- ifelse(model_predictions > 0.75, 1, 0) # Binary classification, cut-off = 0.5
confmatrix_3 <- table(1-data4$v2,1-pred03)
dimnames(confmatrix_3) <- list(Actual = c("Not-survive", "Survive"),
Predicted = c("Not-survive", "Survive"))
confmatrix_3
```


```{r echo=FALSE}
n <- sum(confmatrix_3)
n1_3 <- sum(confmatrix_3[1,])
n2_3 <- sum(confmatrix_3[2,])
accuracy_3 <- (confmatrix_3[1,1]+confmatrix_3[2,2])/n
sensitivity_3 <- confmatrix_3[1,1]/n1_3
specificity_3 <- confmatrix_3[2,2]/n2_3
cat("accuracy: ", accuracy_3)
cat("sensitivity: ", sensitivity_3)
cat("specificity: ", specificity_3)
```
```{r}
threshold <- c("0.5", "0.25", "0.75")
accuracy <- c(accuracy_1, accuracy_2, accuracy_3)
sesnsitivity <- c(sensitivity_1, sensitivity_2, sensitivity_3)
specificity <- c(specificity_1, specificity_2, specificity_3)

acc_results <- data.frame(threshold, accuracy, sesnsitivity, specificity)
kable(acc_results)
```


## 4.
Here we compare the ROC curves for the:
- full model
- our chosen model ("survival"~"Age", "Cancer", "Blood Pressure", "Type of Admission", "Blood PH", "Blood CO2", "Conciousness level")
- and a third, simple model we selected to only use the most significant main effect in our chosen model, conciousness level. ("survival"~"Conciousness level")

```{r, message=FALSE}
#Plot full model ROC
predprob_full<- predict(m_full, type = "response")
full_model_ROC <- roc(data4$v2, predprob_full)
plot(full_model_ROC,legacy.axes = TRUE,print.auc = TRUE)
title("Full model ROC curve\n\n")


#Plot chosen model ROC
chosen_model_ROC <- roc(data4$v2, model_predictions)
plot(chosen_model_ROC,legacy.axes = TRUE,print.auc = T)
title("Chosen model ROC curve\n\n")

#Third Model
simple_model <- glm(v2~v21, family=binomial,data=data4)
simple_model_predictions <- predict(simple_model, type="response")
simple_model_ROC <- roc(data4$v2, simple_model_predictions)
plot(simple_model_ROC, legacy.axes=TRUE, print.auc=TRUE)
title("Simple model ROC curve\n\n")
```
Here we can see that the AUC of the full model is closer to 1 than that of our chosen model, but not by much.
The simple model, using only conciousness level has a much lower AUC then the other two models.

```{r message=FALSE}
model <- c("Full model", "Chosen Model", "Simple model")
AUC <- c(auc(full_model_ROC), auc(chosen_model_ROC), auc(simple_model_ROC))
tab <- data.frame(model, AUC)
kable(tab)
```


## 5.
Here we perform Leave one out cross validation to get a more reliable AUC. It results in the following table.
```{r, message=FALSE, echo=FALSE}
#Create function to perform LOOCV for any model formula
LOOCV <- function(model_formula){
  predprob_LOOCV <- numeric(nrow(data4)) # Create a vector for the predicted values
  for (i in 1:nrow(data4)) {
# Create training and validation sets
    data_training <- data4[-i,]
    data_validation <- data4[i, ,drop=FALSE]
# Fit the model on the training data
    m1<-glm(model_formula, family=binomial,data=data_training)

# Predict the value for the held-out observation
    predprob_LOOCV[i] <- predict(m1, newdata = data_validation, type = "response")
  }
  return(predprob_LOOCV)
}

#Full Model LOOCV AUC
full_model_loocv_auc <- auc(roc(data4$v2, LOOCV(v2~.-v1)))

#Chosen Model LOOCV AUC
chosen_model_loocv_auc <- auc(roc(data4$v2, LOOCV(v2 ~ v3 + v7 + v11 + v14 + v17 + v18 + v21)))

#Third Model LOOCV AUC
simple_model_loocv_auc <- auc(roc(data4$v2, LOOCV(v2~v21)))

#Make Kable
loocv_aucs <- c(full_model_loocv_auc,chosen_model_loocv_auc, simple_model_loocv_auc)
loocv_models <- c("Full Model", "Chosen Model", "Simple Model")
loocv_results <- data.frame(loocv_models, loocv_aucs)
kable(loocv_results)
```
Here we can see that the AUC created by LOOCV is closer to 1 for our chosen model and less for the full model. This is a change from the non-validated AUC calculation.
The simple model still has a much lower AUC than the other two models.
The fact that the full model had a much higher non-validated AUC, than the vlaidated AUC suggests that it was overfitting the data.
Our chosen model however, has a similar AUC between both methods.





# 4:2

## 1
After trying different cp-values, we found that decreasing the cp, will increase model complexity by creating more splits.
This makes sense because the model will only add a split if the model error decreases by at least the cp.
We noticed that "gini" will usually result in a model with fewer splits, than information, for low cp-values.

We ended up choosing a model with "gini" and $cp=0.001$.
We chose "gini" over "information" because it resulted in an easier model to interpret, 
and we chose $cp=0.01$ because we saw that the model didn't change when lowering it further.

```{r echo=FALSE}
library(rpart)
library(rpart.plot)

tm <- rpart(v2 ~.-v1,method = "class", data=data4, parms = list(split = "gini"), cp = 0.001)

rpart.plot(tm, digits=3) #Plot the decision tree
```
This model starts by checkong the v21 parameter (consciousness level). If it's 1 (uncinscious or coma), then the model estimates the probability of death as $0.867$.
If $v21=1$ (awake), then it moves on to the next split, and checks the value of v11 (Blood pressure).
If the blood pressure is higher or equal to 88, then the model estimates the probability of death as $0.667$.
If the blood pressure is lower than 88, then it moves on to the next split, and checks the value of v3 (age).
If the age is lower than 75, then the model estimates the probability of death as $0.079$.
If the age is higher than or equal to 75, the model checks v12 (heart rate).
If the heart rate is lower than 78, the model says the probability of death is $0.000$.
If it's higher, the model checks v11 (Blood pressure) again.
If it's higher or equal to 137, then the probability of death is $0.167$, and otherwise the probability if $0.533$.
## 2.


```{r echo=FALSE}
predprob_tm <- predict(tm, type="prob")[,2]
tm_roc <- roc(data4$v2, predprob_tm)
plot(tm_roc, legacy.axes=TRUE, print.auc=TRUE)
title("Decision tree ROC curve\n\n")
```
Here, we see the ROC curve along with the AUC for the decision tree.

```{r}
model <- c("Decision tree","Full model", "Chosen Model", "Simple model")
AUC <- c(auc(tm_roc), auc(full_model_ROC), auc(chosen_model_ROC), auc(simple_model_ROC))
tab <- data.frame(model, AUC)
kable(tab)
```

We can see the the AUC for our decision tree is slightly lower than the AUC for the full model, and our chosen model.




## 3
```{r}
predprob_LOOCV_tm <- numeric(nrow(data4))

for (i in 1:nrow(data4)) {
# Create training and validation sets
  data_training <- data4[-i,]
  data_validation <- data4[i, ,drop=FALSE]
# Fit the model on the training data
  tm <- rpart(v2 ~.-v1,method = "class", data=data_training, parms = list(split = "information"), cp = 0.0001)


# Predict the value for the held-out observation
  predprob_LOOCV_tm[i] <- predict(tm,newdata = data_validation,type = "prob")[,2]
}

auc(roc(data4$v2,predprob_LOOCV_tm))


```




